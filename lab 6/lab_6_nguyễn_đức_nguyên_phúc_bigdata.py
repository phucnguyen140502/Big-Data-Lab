# -*- coding: utf-8 -*-
"""Lab 6 Nguyễn đức Nguyên Phúc Bigdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18thr7H8BqPdix--uSjRdYGVPYRLR7Il5
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, avg

# Khởi tạo SparkSession
spark = SparkSession.builder.appName("CreditCardFraudDetection").getOrCreate()

from datetime import datetime
from pyspark.sql import Row

# Dữ liệu mô phỏng
data = [
    Row(TransactionID=1, Amount=50, Location="New York", IP_Address="192.168.1.1", Timestamp=datetime(2025,1,29,10,0), DeliveryType="Home"),
    Row(TransactionID=2, Amount=5000, Location="Los Angeles", IP_Address="10.0.0.2", Timestamp=datetime(2025,1,29,10,5), DeliveryType="Store Pickup"),
    Row(TransactionID=3, Amount=30, Location="New York", IP_Address="192.168.1.1", Timestamp=datetime(2025,1,29,10,10), DeliveryType="Home"),
    Row(TransactionID=4, Amount=7000, Location="Texas", IP_Address="10.0.0.3", Timestamp=datetime(2025,1,29,10,15), DeliveryType="PO Box"),
    Row(TransactionID=5, Amount=90, Location="Los Angeles", IP_Address="10.0.0.4", Timestamp=datetime(2025,1,29,10,20), DeliveryType="Home"),
    Row(TransactionID=6, Amount=12000, Location="Texas", IP_Address="10.0.0.5", Timestamp=datetime(2025,1,29,10,25), DeliveryType="Store Pickup"),
    Row(TransactionID=7, Amount=4000, Location="Chicago", IP_Address="172.16.0.1", Timestamp=datetime(2025,1,29,10,30), DeliveryType="Home"),
    Row(TransactionID=8, Amount=80, Location="Chicago", IP_Address="172.16.0.1", Timestamp=datetime(2025,1,29,10,35), DeliveryType="Home"),
    Row(TransactionID=9, Amount=200, Location="Miami", IP_Address="8.8.8.8", Timestamp=datetime(2025,1,29,10,40), DeliveryType="Store Pickup"),
    Row(TransactionID=10, Amount=6000, Location="Seattle", IP_Address="8.8.4.4", Timestamp=datetime(2025,1,29,10,45), DeliveryType="Home")
]

# Tạo DataFrame từ dữ liệu
df = spark.createDataFrame(data)

# Hiển thị 5 dòng đầu tiên
df.show()

df.agg({"Amount":"avg"}).collect()

# Tính trung bình số tiền giao dịch
avg_amount = df.agg({"Amount":"avg"}).collect()[0][0] # Collect the average value
print("Trung bình số tiền giao dịch:", avg_amount)

# Xác định giao dịch lớn hơn 2 lần mức trung bình là đáng nghi
suspicious_transactions = df.filter(col("Amount") > (2 * avg_amount))
suspicious_transactions.show()

df.groupBy("IP_Address").agg(count("*").alias("TransactionCount")).orderBy(col("TransactionCount").desc()).show()

df.groupBy("Location").agg(count("*").alias("TransactionCount")).orderBy(col("TransactionCount").desc()).show()

df.groupBy("DeliveryType").agg(count("*").alias("TransactionCount")).orderBy(col("TransactionCount").desc()).show()

"""#### Machine Learning Analysis"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

# Chỉ lấy các cột số để làm input cho mô hình
features = ["Amount"]
assembler = VectorAssembler(inputCols=features, outputCol="features")

df_features = assembler.transform(df)

# Hiển thị dữ liệu đã được vector hóa
df_features.select("features").show(truncate=False)

# Áp dụng K-Means với 2 cụm (Giao dịch bình thường & Giao dịch đáng ngờ)
kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol("features")
model = kmeans.fit(df_features)

# Dự đoán cụm cho từng giao dịch
df_clustered = model.transform(df_features)

# Hiển thị kết quả
df_clustered.select("TransactionID", "Amount", "prediction").show()

from pyspark.sql.functions import when

df_flagged = df.withColumn("Fraud_Flag",
                           when(col("Amount") > 5000, 1)
                           .when(col("IP_Address") == "8.8.8.8", 1)
                           .otherwise(0))

df_flagged.show()

from pyspark.ml.feature import StringIndexer

# Chuyển đổi dữ liệu danh mục (Location, DeliveryType) thành dạng số
indexer = StringIndexer(inputCols=["Location", "DeliveryType"],
                        outputCols=["LocationIndex", "DeliveryTypeIndex"])
df_indexed = indexer.fit(df_flagged).transform(df_flagged)

assembler = VectorAssembler(inputCols=["Amount", "LocationIndex", "DeliveryTypeIndex"],
                            outputCol="features")
df_ml = assembler.transform(df_indexed).select("features", "Fraud_Flag")
df_ml.show()

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Tạo mô hình Random Forest
rf = RandomForestClassifier(labelCol="Fraud_Flag", featuresCol="features", numTrees=10)

# Chia dữ liệu thành tập train và test
train_data, test_data = df_ml.randomSplit([0.7, 0.3], seed=42)

# Huấn luyện mô hình
rf_model = rf.fit(train_data)

# Dự đoán trên tập test
predictions = rf_model.transform(test_data)

# Đánh giá mô hình
evaluator = BinaryClassificationEvaluator(labelCol="Fraud_Flag")
accuracy = evaluator.evaluate(predictions)
print(f"Accuracy: {accuracy}")

import matplotlib.pyplot as plt

# Trích xuất dữ liệu từ DataFrame
amounts = [row["Amount"] for row in df.collect()]

# Vẽ biểu đồ
plt.hist(amounts, bins=10, edgecolor="black")
plt.xlabel("Transaction Amount")
plt.ylabel("Frequency")
plt.title("Distribution of Transaction Amounts")
plt.show()

